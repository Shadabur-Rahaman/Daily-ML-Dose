# ğŸ¯ Day 28 â€“ Attention Mechanisms  
ğŸ§  #DailyMLDose | Let Your Model Learn What to Focus On

Welcome to **Day 28** of #DailyMLDose!  
Today, we spotlight one of the most powerful innovations in deep learning:  
**Attention Mechanisms** â€” the backbone of modern NLP and Vision models.

---
âœ… Folder Structure
```css

day28-attention-mechanism/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ basic_attention_numpy.py
â”‚   â””â”€â”€ pytorch_scaled_dot_product_attention.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ attention_mechanism_explained.png.png
â”‚   â”œâ”€â”€ scaled_dot_product_attention_formula.png
â”‚   â”œâ”€â”€ attention_score_visualization.png
â”‚   â”œâ”€â”€ soft_vs_hard_attention.png
â”‚   â”œâ”€â”€ attention_flow_diagram.png
â”‚   â”œâ”€â”€ attention_vs_selfattention.png
â”‚   â””â”€â”€ attention_in_machine_translation.png
â””â”€â”€ README.md
```
---
## ğŸ¤” What is Attention?

In human learning, **we donâ€™t read every word equally**.  
We focus more on **key information** â€” and attention lets machines do the same.

---

## ğŸ“Œ Types of Attention

| Type           | Description                               |
|----------------|-------------------------------------------|
| Soft Attention | Uses weighted averages (differentiable)   |
| Hard Attention | Makes discrete choices (non-differentiable)|
| Self-Attention | Each word attends to every other word     |
| Cross-Attention| Target attends to source (e.g. translation) |

ğŸ–¼ï¸ Visuals:  
![Types of Attention](images/attention_mechanism_explained.png.png)  
![Soft vs Hard Attention](images/soft_vs_hard_attention.png)

---

## ğŸ§® Scaled Dot-Product Attention

Given Queries (Q), Keys (K), and Values (V):

```math
Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V
```
ğŸ–¼ï¸ Visuals:


ğŸ§  Self vs Regular Attention
Regular Attention: Decoder focuses on encoder output

Self-Attention: Sequence attends to itself (used in Transformers)

ğŸ–¼ï¸ Visuals:

ğŸŒ Real-World Use Case
In translation:

"The cat sat on the mat." â†’ "Le chat sâ€™est assis sur le tapis."
Each French word focuses differently on English words.
ğŸ–¼ï¸

ğŸ§ª Python Code Demos
âœ… Basic Attention in NumPy
```python
import numpy as np

def attention(Q, K, V):
    d_k = Q.shape[-1]
    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)
    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
    return np.matmul(weights, V)

# Toy input
Q = np.random.rand(1, 5, 64)
K = np.random.rand(1, 5, 64)
V = np.random.rand(1, 5, 64)

output = attention(Q, K, V)
print("Output shape:", output.shape)
```
âœ… Scaled Dot-Product Attention (PyTorch)
```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k**0.5
    weights = F.softmax(scores, dim=-1)
    return torch.matmul(weights, V)

# Example
Q = torch.rand(2, 4, 64)
K = torch.rand(2, 4, 64)
V = torch.rand(2, 4, 64)

output = scaled_dot_product_attention(Q, K, V)
print("Attention Output:", output.shape)
```
ğŸ§  Summary Table
ğŸ§© Component	ğŸ” Description
Q (Query)	What you're looking for
K (Key)	What youâ€™re comparing against
V (Value)	What to return if match is strong
Softmax(QÂ·Káµ€)	Attention weights
Output = weighted V	Final attended representation

ğŸ” Previous Post
![ğŸ“Œ Day 27 â†’ RNN / LSTM / GRU](.//day27-rnn-lstm-gru)

ğŸ™Œ Stay Connected
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
â­ Star the GitHub Repo
Train your models to focus â€” thatâ€™s where intelligence begins. ğŸ¯
