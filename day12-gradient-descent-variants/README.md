# ğŸ› ï¸ Day 12 â€“ Gradient Descent Variants: SGD, RMSProp, Adam

Welcome to **Day 12** of #DailyMLDose!

Today weâ€™re diving into the optimization engine that powers most modern machine learning models â€” **Gradient Descent**, and its most powerful variants: **SGD**, **RMSProp**, and **Adam**.

---

## ğŸ“Œ What is Gradient Descent?
Gradient Descent is an optimization technique that minimizes a loss function by iteratively updating model weights in the direction of the steepest descent.

w := w - learning_rate * gradient


Itâ€™s how neural networks learn â€” one tiny step at a time.

---

ğŸ“‚ Folder Structure â€“ `day12-gradient-descent-variants/`
```
day12-gradient-descent-variants/
â”œâ”€â”€ README.md
â”œâ”€â”€ code/
â”‚   â””â”€â”€ gradient_descent_variants.py
â””â”€â”€ images/
    â”œâ”€â”€ adam-optimization-visual.webp
    â”œâ”€â”€ adam-optimization.webp
    â”œâ”€â”€ Gradient-Descent-graph.webp
    â”œâ”€â”€ Gradient-Descent-visual.png
    â”œâ”€â”€ Gradient_Descent_Variants.webp
    â”œâ”€â”€ RMSProp_3d_graph.png
    â”œâ”€â”€ RMSProp_visual.png
    â”œâ”€â”€ Stochastic_Gradient_Descent_graph.webp
    â””â”€â”€ Stochastic_Gradient_Descent_visual.png
```
---

## ğŸ” Popular Variants of Gradient Descent

ğŸ“Œ **Batch Gradient Descent**
- Updates weights after computing gradient on the **entire dataset**.
- Very accurate but slow and memory-intensive.

ğŸ“Œ **Stochastic Gradient Descent (SGD)**
- Updates weights after each training sample (1 sample per update).
- Faster but high variance â†’ noisy path.

ğŸ“¸ Visuals:
![SGD Graph](images/Stochastic_Gradient_Descent_graph.webp)  
![SGD Visual](images/Stochastic_Gradient_Descent_visual.png)

ğŸ“Œ **Mini-Batch Gradient Descent**
- Updates after small batches (e.g., 32 or 64 samples).
- Efficient, stable, and the go-to for deep learning.

ğŸ“Œ **RMSProp**
- Adapts learning rates using a **moving average of squared gradients**.
- Handles non-stationary objectives and vanishing gradients.

ğŸ“¸ Visuals:
![RMSProp 3D](RMSProp_3d_graph.png)  
![RMSProp Visual](RMSProp_visual.png)

ğŸ“Œ **Adam Optimizer**
- Combines **Momentum** and **RMSProp**.
- Maintains moving averages of gradient (1st moment) and squared gradient (2nd moment).
- Default optimizer in many DL frameworks (PyTorch, TensorFlow).

ğŸ“¸ Visuals:
![Adam](adam-optimization.webp)  
![Adam Flow](adam-optimization-visual.webp)

---

## ğŸ¯ Concept Visualizations
![Gradient Descent Family](Gradient_Descent_Variants.webp)  
![Descent Path Visual](Gradient-Descent-visual.png)  
![Cost Surface](Gradient-Descent-graph.webp)

---

## ğŸ§  Summary Table
```
| Optimizer      | Update Speed | Path Variability | Learning Rate Adaptation | Best For                  |
|----------------|--------------|------------------|---------------------------|---------------------------|
| Batch GD       | Slow         | Low              | âŒ                        | Small datasets            |
| SGD            | Fast         | High             | âŒ                        | Real-time/streaming tasks |
| Mini-Batch     | Moderate     | Medium           | âŒ                        | Deep learning models      |
| RMSProp        | Fast         | Smooth           | âœ…                        | RNNs, non-stationary data |
| Adam           | Fastest      | Stable           | âœ…                        | Deep learning & NLP       |
```
---

## âœ… Real-World Benefits
- Faster convergence in deep models  
- Better performance in noisy and sparse data  
- Handles exploding or vanishing gradients  

---

## ğŸ” Previous:
[Day 11 â†’ One-Hot vs Label Encoding](../day11-encoding)

---

## ğŸ¨ Visual Credits:
- Adam/RMSProp Visuals: [@ml_insights](https://x.com/ml_insights)  
- Optimizer Summary Charts: [@ml_diagrams](https://x.com/ml_diagrams)  
- SGD Graphs: [@deeplearning_ai](https://x.com/deeplearning_ai)  

---

ğŸ“Œ Stay Connected:  
- â­ Star the GitHub Repo  
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249/)  

Letâ€™s optimize our learning â€” one gradient step at a time! ğŸš€
