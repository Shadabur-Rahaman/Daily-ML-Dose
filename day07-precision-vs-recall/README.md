# ğŸ¯ Day 7 â€“ Precision vs Recall Tradeoff in Machine Learning

Welcome to **Day 7** of #DailyMLDose!

Today we dive into the important tradeoff between **Precision** and **Recall** â€” two powerful metrics often at odds in classification problems.

---

## ğŸ“Œ Definitions

- **Precision** = TP / (TP + FP)  
  â†’ Of all predicted positives, how many are correct?

- **Recall** = TP / (TP + FN)  
  â†’ Of all actual positives, how many were caught?

---
### ğŸ—‚ï¸ Folder Structure â€“ 
```
day07-precision-vs-recall/
â”œâ”€â”€ README.md
â”œâ”€â”€ precision_vs_recall_curve.py        # Python demo script
â”œâ”€â”€ precision_recall_curve.png          # Matplotlib/seaborn visual
â””â”€â”€ precision_vs_recall_tradeoff.png    # Concept illustration
---
## ğŸ¯ The Tradeoff

Improving one often hurts the other:
- Increasing **Recall** may lower **Precision** (more false positives)
- Increasing **Precision** may lower **Recall** (more false negatives)

ğŸ“Š You often tune thresholds (e.g., model.predict_proba > 0.7) to balance them.

---

## ğŸ” When to Prioritize

| Scenario                                | Focus On    |
|-----------------------------------------|-------------|
| Fraud Detection                         | Recall      |
| Spam Detection                          | Precision   |
| Medical Diagnosis (e.g., cancer)        | Recall      |
| Search Engines (relevant results)       | Precision   |

---

## ğŸ§ª Python Code â€“ Precision-Recall Curve

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt

X, y = make_classification(n_samples=1000, weights=[0.85, 0.15], random_state=42)
model = LogisticRegression()
model.fit(X, y)

y_scores = model.predict_proba(X)[:, 1]
precision, recall, thresholds = precision_recall_curve(y, y_scores)
pr_auc = auc(recall, precision)

plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()
```
ğŸ–¼ï¸ Output:

ğŸ“ Visual Concept

A high threshold = High Precision, Low Recall

A low threshold = High Recall, Low Precision

ğŸ§  Summary
Use Precision when false positives are costly

Use Recall when false negatives are dangerous

Use F1-Score when you need a balanced approach

ğŸ” Previous:
Day 6 â†’ Confusion Matrix

ğŸ”” Stay Updated

â­ Star the GitHub repo

 [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249/)  
