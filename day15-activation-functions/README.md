# âš¡ Day 15 â€“ Activation Functions in ML: ReLU, Sigmoid, Tanh, GELU

Welcome to **Day 15** of #DailyMLDose!

Activation functions are what make neural networks **non-linear** and **powerful**. They help your model learn complex patterns, encode probabilities, and allow deep networks to stack multiple layers.

---

## ğŸ“Œ What Are Activation Functions?

An **activation function** determines whether a neuron should be activated or not by applying a mathematical transformation to the weighted input.

They are essential to:
- Introduce **non-linearity**
- Enable **backpropagation**
- Scale outputs within a certain range

---

ğŸ“‚ Folder Structure â€“ `day15-activation-functions/`
```
day15-activation-functions/
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ Activation-function-pros-and-cons-1.webp
â”‚ â”œâ”€â”€ activation_functions.jpg
â”‚ â”œâ”€â”€ activation_functions_flow_diagram.png
â”‚ â”œâ”€â”€ Different-types-of-activation-functions-a-Sigmoid-b-ReLU-c-Tanh-and-d-GELU.png
â”‚ â””â”€â”€ How-to-Choose-an-Output-Layer-Activation-Function.webp
â”œâ”€â”€ code/
â”‚ â””â”€â”€ activation_functions_demo.py
â””â”€â”€ README.md
```

---

## ğŸ” Key Activation Functions

### ğŸ”¸ **ReLU (Rectified Linear Unit)**
- Formula: `f(x) = max(0, x)`
- Pros: Simple, fast, works well with deep networks
- Cons: Can "die" if neurons output only 0

### ğŸ”¸ **Sigmoid**
- Formula: `f(x) = 1 / (1 + e^-x)`
- Pros: Probabilistic output (0 to 1)
- Cons: Vanishing gradients for large inputs

### ğŸ”¸ **Tanh**
- Formula: `f(x) = (e^x - e^-x) / (e^x + e^-x)`
- Pros: Output between -1 and 1 (zero-centered)
- Cons: Still suffers from vanishing gradient

### ğŸ”¸ **GELU (Gaussian Error Linear Unit)**
- Formula: `f(x) = x * Î¦(x)` where Î¦ is the CDF of the normal distribution
- Pros: Smooth like tanh, powerful for Transformers
- Cons: More computationally intensive

---

## ğŸ§  Visual Comparison

ğŸ“Š  
![Function Charts](images/activation_functions.jpg)  
![Flow Diagram](images/activation_functions_flow_diagram.png)  
![Multiple Functions](images/Different-types-of-activation-functions-a-Sigmoid-b-ReLU-c-Tanh-and-d-GELU.png)  
![Choosing Output Layer](images/How-to-Choose-an-Output-Layer-Activation-Function.webp)  
![Pros & Cons Table](images/Activation-function-pros-and-cons-1.webp)

---

## ğŸ§ª Python Demo

See [`activation_functions_demo.py`](code/activation_functions_demo.py) for code examples of each function using NumPy and PyTorch.

---

## ğŸ§© Summary Table

| Activation | Range       | Non-Linearity | Zero-Centered | Usage                        |
|------------|-------------|----------------|---------------|------------------------------|
| Sigmoid    | 0 to 1      | âœ…             | âŒ            | Binary classification        |
| Tanh       | -1 to 1     | âœ…             | âœ…            | Hidden layers (early NN)     |
| ReLU       | 0 to âˆ      | âœ…             | âŒ            | CNNs, deep learning models   |
| GELU       | Varies      | âœ…             | âœ…            | Transformers, NLP models     |

---

## ğŸ” Previous:
[Day 14 â†’ Loss Functions: MSE, BCE, Cross-Entropy](../day14-loss-functions)

---

## ğŸ¨ Visual Credits:
- Comparison Charts: @ml_diagrams  
- Flow Visuals: @pythonengineer, @analyticsvidhya  
- Function Curves: @sebastianraschka

---

ğŸ“Œ Stay Connected:
- â­ Star the GitHub Repo  
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)

Make your neurons fire right! Letâ€™s keep learning. ğŸš€
