# ğŸŒ³ Day 16 â€“ Decision Trees & Gini vs Entropy

Welcome to **Day 16** of #DailyMLDose!

Todayâ€™s focus: the classic, interpretable, and surprisingly powerful model â€” **Decision Trees**.  
We'll break down how they split data using **Gini Impurity** and **Entropy**.

---

## ğŸ“Œ What Is a Decision Tree?

A **Decision Tree** is a supervised ML model used for both **classification** and **regression**. It splits the dataset into branches by asking the most informative questions.

At each node:
- It evaluates a **feature & threshold**
- Chooses a **split criterion**: either **Gini Impurity** or **Entropy**
- Continues recursively until stopping criteria are met

---

ğŸ“‚ Folder Structure â€“ `day16-decision-trees/`
```
day16-decision-trees/
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ decision_tree_structure.png
â”‚ â”œâ”€â”€ gini_vs_entropy_chart.webp
â”‚ â”œâ”€â”€ entropy_formula.png
â”‚ â”œâ”€â”€ gini_formula.png
â”‚ â”œâ”€â”€ decision_tree_diagram_flow.png
â”‚ â””â”€â”€ decision_tree_cart_vs_id3.png
â”œâ”€â”€ code/
â”‚ â””â”€â”€ decision_tree_gini_entropy_demo.py
â””â”€â”€ README.md
```

---

## ğŸ§  Gini vs Entropy â€“ Whatâ€™s the Difference?

| Criteria  | Gini Impurity                         | Entropy (Information Gain)             |
|-----------|----------------------------------------|----------------------------------------|
| Formula   | 1 - Î£(páµ¢Â²)                             | -Î£(páµ¢ logâ‚‚ páµ¢)                         |
| Range     | [0, 0.5]                               | [0, 1]                                 |
| Speed     | Faster (no log calculation)           | Slightly slower                        |
| Use Case  | CART (Classification & Regression Tree)| ID3, C4.5                              |
| Output    | Degree of impurity                    | Uncertainty or information gain        |

ğŸ“¸  
![Formulas](images/gini_formula.png)  
![Entropy](images/entropy_formula.png)  
![Visual Comparison](images/gini_vs_entropy_chart.png)

---

## ğŸŒ¿ How a Decision Tree Works (Simplified)

1. At each node, it evaluates all possible splits
2. Measures impurity using **Gini** or **Entropy**
3. Chooses the split that reduces impurity the most
4. Repeats the process recursively

ğŸ“Š  
![Tree Flow](images/decision_tree_diagram_flow.png)

---

## ğŸ” Real-World Example in Python

See [`decision_tree_gini_entropy_demo.py`](code/decision_tree_gini_entropy_demo.py)

---

## ğŸ§ª Pros & Cons

| Pros                       | Cons                          |
|----------------------------|-------------------------------|
| Easy to interpret & visualize | Prone to overfitting         |
| No need for scaling        | Can be unstable with small data |
| Handles both numerical & categorical data | Biased toward dominant classes |

---

## ğŸ” Previous:
[Day 15 â†’ Activation Functions: ReLU, Sigmoid, Tanh, GELU](../day15-activation-functions)

---

## ğŸ¨ Visual Credits:
- Decision Tree Charts: @ml_diagrams  
- Gini/Entropy Comparison: @sebastianraschka  
- Flow Visuals: @codingninjas

---

ğŸ“Œ Stay Connected:
- â­ Star the GitHub Repo  
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)

Letâ€™s keep branching out our ML knowledge! ğŸŒ¿
