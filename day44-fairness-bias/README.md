# ğŸš€ Day 44 â€“ Fairness and Bias in Machine Learning  
**#DailyMLDose** | Ensuring Equity in AI Models

Machine learning models are powerfulâ€”but with power comes responsibility. Bias in data or algorithms can lead to unfair, even harmful, decisions. Today, weâ€™ll explore how to detect, evaluate, and mitigate bias to build more trustworthy AI systems.

---

## ğŸ” Overview  
Today we explore:

- ğŸ§  Understanding Bias in ML  
- âš–ï¸ Types of Bias (Label, Selection, Measurement)  
- ğŸ“Š Fairness Metrics  
- ğŸ› ï¸ Bias Mitigation Techniques  
- ğŸ§ª Testing Model Fairness  
- ğŸ” Tools: AIF360, Fairlearn  
- ğŸŒ Ethical Implications of ML  
- ğŸ” Post-deployment Monitoring

---

## ğŸ–¼ï¸ Visuals

### 1. Sources of Bias in ML Pipeline  
<img src="images/bias_sources_pipeline.png" />

---

### 2. Fairness Metrics Cheat Sheet  
<img src="images/fairness_metrics.png" width="650"/>

---

### 3. Pre vs In vs Post Processing Techniques  
<img src="images/mitigation_methods.png" width="600"/>

---

### 4. Real-world Case Studies of Bias  
<img src="images/case_studies_bias.png" width="650"/>

---

## ğŸ§ª Code Highlights

### âœ… 1. Check for Demographic Parity with Fairlearn

```python
from fairlearn.metrics import demographic_parity_difference
from sklearn.metrics import accuracy_score

dp_diff = demographic_parity_difference(y_true, y_pred, sensitive_features=gender)
print(f"Demographic Parity Difference: {dp_diff:.3f}")
```
âœ… 2. Bias Mitigation using AIF360 Reweighing

```python
 
from aif360.algorithms.preprocessing import Reweighing
from aif360.datasets import BinaryLabelDataset

RW = Reweighing(unprivileged_groups, privileged_groups)
dataset_transf = RW.fit_transform(dataset_orig_train)
```
âœ… 3. Serialize and Load Fair Model

```python
 
import joblib
joblib.dump(fair_model, 'fair_model.pkl')
model = joblib.load('fair_model.pkl')
```
âœ… 4. GitHub Action for Fairness Testing

```yaml
 
name: Bias Check CI

on: [push]

jobs:
  test-bias:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    - name: Install dependencies
      run: pip install -r requirements.txt
    - name: Check fairness metrics
      run: python test_fairness.py
```
âœ… 5. Monitor Disparity Index via Prometheus

```python
 
from prometheus_client import Gauge

disparity_index = Gauge('disparity_index', 'Bias across demographic groups')
disparity_index.set(dp_diff)
```
ğŸ“ Folder Structure

```css
 
ğŸ“ day44-fairness-bias/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ check_fairness.py
â”‚   â”œâ”€â”€ mitigate_bias.py
â”‚   â”œâ”€â”€ serialize_fair_model.py
â”‚   â”œâ”€â”€ github_fairness_ci.yml
â”‚   â””â”€â”€ prometheus_fairness_monitor.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ bias_sources_pipeline.png
â”‚   â”œâ”€â”€ fairness_metrics.png
â”‚   â”œâ”€â”€ mitigation_methods.png
â”‚   â””â”€â”€ case_studies_bias.png
â””â”€â”€ README.md
