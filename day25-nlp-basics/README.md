# ğŸ“š Day 25 â€“ NLP Basics: TF-IDF & Word2Vec  
ğŸ§  #DailyMLDose | Giving Meaning to Language with Math

Welcome to **Day 25** of #DailyMLDose!  
Letâ€™s explore the backbone of modern Natural Language Processing: **Text Vectorization** through **TF-IDF** and **Word2Vec**.  
> How do we convert messy text into numerical signals that machines can learn from? Letâ€™s decode it!

---

## ğŸ§¾ What is NLP?

Natural Language Processing (NLP) enables machines to **understand, interpret, and generate** human language.  
It powers chatbots ğŸ¤–, search engines ğŸ”, sentiment analysis â¤ï¸, and more.

### ğŸ§  NLP Applications
![NLP Used For](images/What-is-Natural-Language-Processing-NLP-Used-For-1.jpg)

---

## ğŸ› ï¸ NLP Pipeline Overview

![NLP Pipeline](images/NLP_pipeline.png)  
![NLP Architecture](images/NLP_architecture.png)

---

## ğŸ”¢ 1. TF-IDF: Term Frequencyâ€“Inverse Document Frequency

TF-IDF scores highlight **how important a word is** in a document relative to a corpus.

### ğŸ§  Formula:
- **TF(w, d)** = frequency of word `w` in document `d`
- **IDF(w)** = log(total docs / docs containing w)

### ğŸ” Visuals:
![TF-IDF Explained](images/tf-idf-explained.png)  
![TF-IDF Vectorization](images/tf-idf-vectorization.png)  
![TF-IDF Demo](images/tfidf_vectorization_demo.png)

---

## ğŸ§  2. Word2Vec: Dense Semantic Embeddings

Rather than counting words, Word2Vec **learns relationships** between words via context.

ğŸ“Œ Trained using either:
- **Skip-Gram**: predict context from center word  
- **CBOW**: predict center word from context

### ğŸ“Š Visuals:
![Word2Vec Architecture](images/word2vec_architecture.png)  
![Training Visual](images/ord2vec_training_visual.png)  
![Word Embedding Space](images/word_embeddings_space_demo.png)  
![Word2Vec Network](images/network-of-word2vec.webp)

---

## ğŸ” Bonus Visual: NLP in Action  
![Projects NLP](images/do-nlp-sentiment-analysis-and-topic-modeling-projects.png)

---

## ğŸ§ª Code Examples

### âœ… TF-IDF with `sklearn`

```python
from sklearn.feature_extraction.text import TfidfVectorizer

docs = ["this is a good book", "this book is good", "this is a bad book"]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(docs)

print(vectorizer.get_feature_names_out())
print(X.toarray())
âœ… Word2Vec with gensim
python
Copy
Edit
from gensim.models import Word2Vec

sentences = [["machine", "learning", "is", "fun"],
             ["deep", "learning", "is", "a", "part", "of", "ml"]]

model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, sg=1)
vector = model.wv["learning"]
print("Vector for 'learning':", vector[:5])
```
ğŸ“‚ Folder Structure
```
day25-nlp-basics/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ tfidf_example.py
â”‚   â””â”€â”€ word2vec_example.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ do-nlp-sentiment-analysis-and-topic-modeling-projects.png
â”‚   â”œâ”€â”€ network-of-word2vec.webp
â”‚   â”œâ”€â”€ NLP_architecture.png
â”‚   â”œâ”€â”€ NLP_pipeline.png
â”‚   â”œâ”€â”€ ord2vec_training_visual.png
â”‚   â”œâ”€â”€ tf-idf-explained.png
â”‚   â”œâ”€â”€ tf-idf-vectorization.png
â”‚   â”œâ”€â”€ tfidf_vectorization_demo.png
â”‚   â”œâ”€â”€ What-is-Natural-Language-Processing-NLP-Used-For-1.jpg
â”‚   â”œâ”€â”€ word2vec_architecture.png
â”‚   â””â”€â”€ word_embeddings_space_demo.png
```  
ğŸ§  Summary
Technique	Purpose	Output	Common Use
TF-IDF	Statistical importance	Sparse Vectors	Document classification, search
Word2Vec	Semantic proximity	Dense Embeddings	Sentiment, NER, word similarity

ğŸ” Previous Post
![ğŸ“Œ Day 24 â†’ Transfer Learning](.//day24-transfer-learning)


ğŸ™Œ Stay Connected
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
â­ Star the GitHub Repo
From words to vectors â€” weâ€™re teaching machines to speak our language. ğŸ§ ğŸ’¬

