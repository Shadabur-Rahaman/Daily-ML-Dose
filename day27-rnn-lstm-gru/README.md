# ğŸ” Day 27 â€“ RNN / LSTM / GRU  
ğŸ§  #DailyMLDose | Understanding Sequence Models

Welcome to **Day 27** of #DailyMLDose!  
Today, we revisit the fundamental neural architectures that were state-of-the-art before Transformers took over:  
**RNNs, LSTMs, and GRUs** â€“ the OGs of sequence modeling. ğŸ“ˆ

---

## ğŸ”„ Why Sequence Models?

Theyâ€™re designed for data where **order matters**:  
ğŸ“ Text â†’ Language Modeling  
ğŸ“Š Time Series â†’ Stock Prediction  
ğŸµ Audio â†’ Speech Recognition

---
âœ… Folder Structure
```css

day27-rnn-lstm-gru/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ rnn_demo.py
â”‚   â”œâ”€â”€ lstm_demo.py
â”‚   â””â”€â”€ gru_demo.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ rnn_vs_lstm_vs_gru_comparison.png
â”‚   â”œâ”€â”€ rnn_loop_architecture.png
â”‚   â”œâ”€â”€ lstm_cell_explained.png
â”‚   â”œâ”€â”€ gru_cell_explained.png
â”‚   â”œâ”€â”€ rnn_exploding_gradient.png
â”‚   â””â”€â”€ sequence_modeling_example_diagram.png
â””â”€â”€ README.md
```
---

## ğŸ§© 1. RNN â€“ Recurrent Neural Network

- Processes inputs **sequentially**
- Passes hidden state forward
- Learns time-dependent patterns

**BUT:** Suffers from vanishing/exploding gradients

ğŸ–¼ï¸ Visual:  
![RNN Loop](images/rnn_loop_architecture.png)  
![Exploding Gradient](images/rnn_exploding_gradient.png)

---

## ğŸ§  2. LSTM â€“ Long Short-Term Memory

- Adds **gates** (input, forget, output)  
- Retains long-range dependencies  
- Solves gradient issues

ğŸ–¼ï¸ Visual:  
![LSTM Cell](images/lstm_cell_explained.jpg)

---

## âš¡ 3. GRU â€“ Gated Recurrent Unit

- Simplified version of LSTM  
- Combines gates into fewer components  
- Faster, competitive performance

ğŸ–¼ï¸ Visual:  
![GRU Cell](images/gru_cell_explained.jpg)

---

## ğŸ§  Summary Comparison

| Feature            | RNN        | LSTM         | GRU          |
|--------------------|------------|--------------|--------------|
| Vanishing Gradient | âŒ Yes     | âœ… Resolved  | âœ… Resolved  |
| Long-Term Memory   | âŒ Weak    | âœ… Strong    | âœ… Strong    |
| Training Speed     | âš¡ Fast    | ğŸ¢ Slower    | âš¡ Faster    |
| Architecture       | Simple     | Complex      | Medium       |

ğŸ–¼ï¸ Comparison Visual:  
![Comparison](images/rnn_vs_lstm_vs_gru_comparison.png)

---

## ğŸ§ª Code Snippets

### ğŸ” RNN with PyTorch

```python
import torch.nn as nn

rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1, batch_first=True)
```
ğŸ” LSTM
```python

lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=1, batch_first=True)
```
ğŸ” GRU
```python

gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1, batch_first=True)
```
ğŸ–¼ï¸ Application Visual

ğŸ” Previous Post
1[ğŸ“Œ Day 26 â†’ Transformers](.//day26-transformers)

ğŸ™Œ Stay Connected
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
â­ Star the GitHub Repo
Memory matters. Let your models remember wisely. ğŸ§ ğŸŒ€
