# âš–ï¸ Day 30 â€“ Imbalanced Data Techniques (SMOTE, Class Weights)  
ğŸ§  #DailyMLDose | Giving Minority Classes a Fair Chance

Welcome to **Day 30** of #DailyMLDose!  
Today we explore how to tackle one of the most common real-world problems in ML â€” **imbalanced datasets**.

Whether it's fraud detection, medical diagnosis, or spam filtering â€” the **minority class matters**!

---
âœ… Folder Structure
```css
day30-imbalanced-data-techniques/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ smote_demo.py
â”‚   â””â”€â”€ class_weight_sklearn.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ smote_visualization.png
â”‚   â”œâ”€â”€ class_weight_explained.png
â”‚   â”œâ”€â”€ data_distribution_before_after.png
â”‚   â”œâ”€â”€ imbalance_problem_diagram.png
â”‚   â”œâ”€â”€ evaluation_metrics_imbalance.jpg
â”‚   â””â”€â”€ confusion_matrix_imbalanced.png
â””â”€â”€ README.md
```
---
## ğŸ“‰ The Problem

When one class heavily outweighs the other(s), most models tend to predict the **majority class** all the time.  
Accuracy becomes misleading and F1 Score, Precision, and Recall become more important.

ğŸ–¼ï¸ Visuals:  
![Imbalance](images/imbalance_problem_diagram.png)  
![Metrics](images/evaluation_metrics_imbalance.jpg)

---

## ğŸ§ª Technique 1: SMOTE (Synthetic Minority Over-sampling Technique)

ğŸ“Œ Creates synthetic examples of the minority class using k-NN interpolation  
âœ… More balanced training set  
âš ï¸ Risk of overfitting if used improperly

ğŸ–¼ï¸ Visuals:  
![SMOTE](images/smote_visualization.png)  
![Distribution](images/data_distribution_before_after.png)

---

## ğŸ§ª Technique 2: Class Weights

ğŸ“Œ Tells the model to **"pay more attention"** to the minority class  
âœ… Works great with tree models and neural networks  
ğŸ’¡ Many libraries support `class_weight='balanced'`

ğŸ–¼ï¸ Visual:  
![Class Weight](images/class_weight_explained.png)

---

## ğŸ”¬ Evaluation Metrics for Imbalanced Data

- **Precision**: Of all predicted positives, how many were correct?
- **Recall**: Of all actual positives, how many did we catch?
- **F1 Score**: Balance between precision and recall
- **Confusion Matrix**: Deeper insight

ğŸ–¼ï¸ Confusion Matrix:  
![Confusion Matrix](images/confusion_matrix_imbalanced.png)

---

## ğŸ§ª Code Demos

### ğŸ” SMOTE Example (Imbalanced-learn)

```python
from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification

X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)

smote = SMOTE()
X_res, y_res = smote.fit_resample(X, y)

print("Before SMOTE:", dict(zip(*np.unique(y, return_counts=True))))
print("After SMOTE:", dict(zip(*np.unique(y_res, return_counts=True))))
```
âš–ï¸ Class Weight in Sklearn
```python
from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.array([0, 1])
weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)
print("Class Weights:", dict(zip(classes, weights)))

model = LogisticRegression(class_weight='balanced')
model.fit(X, y)
```
âœ… Summary Table
Technique	When to Use	Pros	Cons
SMOTE	Low-data scenarios	Boosts minority signal	Risk of overfitting
Class Weights	Most models	Easy & effective	May underperform on noisy data
Undersampling	Large majority class	Fast	Risk of losing information

ğŸ” Previous Post
ğŸ“Œ Day 29 â†’ Time Series Forecasting (ARIMA, LSTM)

ğŸ¨ Visual Credits
@imbalanced-learn, @Scikit-learn, @KDnuggets, @TowardsDataScience

ğŸ™Œ Thank You for Joining #DailyMLDose ğŸ‰
ğŸ”— Follow Shadabur Rahaman
â­ Star the GitHub Repo
Letâ€™s keep fixing biases â€” in data and in life. ğŸ’¡
