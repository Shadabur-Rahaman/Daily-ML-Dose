# ğŸ“‰ Day 14 â€“ Loss Functions in Machine Learning: MSE, BCE, Cross-Entropy

Welcome to **Day 14** of #DailyMLDose!

Loss functions measure how far your model's predictions are from the actual values â€” they are the **core signal** for model training.

---

## ğŸ“Œ Why Loss Functions Matter?

Loss functions tell your model *how wrong* it is.

The optimizer uses the **gradient of the loss** to update weights and improve performance during training. Choosing the right loss function is **crucial for task-specific success**.

---

ğŸ“‚ Folder Structure â€“ `day14-loss-functions/`
```
day14-loss-functions/
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ bce_visuals.png
â”‚ â”œâ”€â”€ bse_formula.webp
â”‚ â”œâ”€â”€ cross_entropy.webp
â”‚ â”œâ”€â”€ cross_entropy_formula.webp
â”‚ â”œâ”€â”€ cross_entropy_visuals.webp
â”‚ â”œâ”€â”€ loss_functions_explained.png
â”‚ â”œâ”€â”€ mse_formula.png
â”‚ â””â”€â”€ mse_visuals.png
â”œâ”€â”€ code/
â”‚ â””â”€â”€ loss_functions_demo.py
â””â”€â”€ README.md
```


---

## ğŸ” Types of Loss Functions

### ğŸŸ© Mean Squared Error (MSE)
- Common in **regression** tasks  
- Measures squared difference between prediction and ground truth  
- Penalizes large errors more heavily

ğŸ“¸  
![MSE Formula](images/mse_formula.png)  
![MSE Visuals](images/mse_visuals.png)

---

### ğŸŸ¥ Binary Cross Entropy (BCE)
- Used for **binary classification**
- Penalizes confident wrong predictions heavily

ğŸ“¸  
![BCE Visual](images/bce_visuals.png)  
![BCE Formula](images/bse_formula.webp)

---

### ğŸŸ¦ Cross-Entropy Loss
- Used for **multi-class classification**
- Measures the dissimilarity between two probability distributions

ğŸ“¸  
![Cross Entropy](images/cross_entropy.webp)  
![Cross Entropy Formula](images/cross_entropy_formula.webp)  
![Cross Entropy Visuals](images/cross_entropy_visuals.webp)

---

## ğŸ§  Visual Summary

ğŸ“Š  
![All Loss Functions Explained](images/loss_functions_explained.png)

---

## ğŸ§ª Python Demo

See [`loss_functions_demo.py`](code/loss_functions_demo.py) for code comparing these loss functions on toy datasets.

---

## ğŸ§© Quick Reference Table

| Loss Function     | Task Type            | Sensitivity         | Formula Summary                        |
|-------------------|----------------------|----------------------|----------------------------------------|
| MSE               | Regression           | Large errors         | (y_pred - y_true)^2                    |
| BCE               | Binary Classification| Confident wrong preds| -[y log(p) + (1-y) log(1-p)]           |
| Cross Entropy     | Multi-class Classification | Log loss     | -Î£ y log(p)                            |

---

## ğŸ” Previous:
[Day 13 â†’ Regularization: L1, L2, ElasticNet](../day13-regularization)

---

## ğŸ¨ Visual Credits:
- BCE/MSE Graphs: @ml_diagrams  
- Cross Entropy Diagrams: @seeingthecode  
- Summary Charts: @vijaykrishna101

---

ğŸ“Œ Stay Connected:
- â­ Star this GitHub Repo  
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)

Train loss? Validation loss? Now you know what they really mean. ğŸ’¡  
Letâ€™s keep optimizing! ğŸš€
