# ğŸ§  Day 31 â€“ Autoencoders: Learn to Compress & Reconstruct  
ğŸ¯ #DailyMLDose (Bonus Post) | Unsupervised Learning Powerhouses

Welcome to **Day 31** of #DailyMLDose!  
Today we explore **Autoencoders** - neural networks that learn efficient data representations through compression and reconstruction.

---

## ğŸš€ What are Autoencoders?

Autoencoders are unsupervised neural networks that:
1. **Compress** input data into a latent-space representation
2. **Reconstruct** the original input from this compressed form

### ğŸ’¡ Think of it like:
> An archivist who summarizes complex documents into concise abstracts,  
> then reconstructs the full document from those summaries. ğŸ“„â¡ï¸ğŸ“â¡ï¸ğŸ“„

---

## ğŸ¯ When to Use Autoencoders

âœ… Dimensionality reduction  
âœ… Anomaly detection  
âœ… Image denoising  
âœ… Feature extraction  
âœ… Data generation (with variational autoencoders)

---

## ğŸ”‘ Key Components

| Component         | Function                                      | Importance |
|-------------------|-----------------------------------------------|------------|
| **Encoder**       | Compresses input â†’ latent space (bottleneck)  | Learns efficient representations |
| **Latent Space**  | Compressed knowledge representation           | Critical information bottleneck |
| **Decoder**       | Reconstructs input from latent representation | Tests representation quality |

---

## ğŸ§© Types of Autoencoders

| Type                | Special Feature                       | Use Case                  |
|---------------------|----------------------------------------|---------------------------|
| **Undercomplete**   | Bottleneck layer smaller than input    | Feature learning          |
| **Denoising**       | Trained on corrupted inputs           | Data cleaning             |
| **Variational (VAE)**| Probabilistic latent space            | Data generation           |
| **Sparse**          | Activation constraints                | Interpretable features    |

---

## ğŸ–¼ï¸ Autoencoder Visualizations

<div align="center">

### ğŸ—ï¸ Basic Architecture  
![Architecture](images/autoencoder_architecture.png)  

### ğŸ” Input vs Reconstruction  
![Reconstruction](images/input_vs_reconstruction.png)  

### âš ï¸ Anomaly Detection  
![Anomaly](images/anomaly_detection_autoencoder.png)  

### ğŸŒ€ Latent Space Visualization  
![Latent Space](images/autoencoder_latent_space_2D.png)  

### ğŸ“‰ Training Progress  
![Loss Curve](images/reconstruction_loss_curve.png)  

### âœ¨ Denoising Effect  
![Denoising](images/denoising_effect_mnist.png)  
</div>

---

## ğŸ§ª Code Examples

### Basic Autoencoder (Keras)
```python
from tensorflow.keras import layers, Model

# Encoder
input_img = layers.Input(shape=(784,))
encoded = layers.Dense(128, activation='relu')(input_img)
encoded = layers.Dense(64, activation='relu')(encoded)
encoded = layers.Dense(32, activation='relu')(encoded)  # Latent space

# Decoder
decoded = layers.Dense(64, activation='relu')(encoded)
decoded = layers.Dense(128, activation='relu')(decoded)
decoded = layers.Dense(784, activation='sigmoid')(decoded)

# Autoencoder model
autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
```
Denoising Autoencoder
```python
# Corrupt input with noise
noisy_imgs = original_imgs + 0.5 * np.random.normal(size=original_imgs.shape)

# Train to reconstruct original from noisy input
autoencoder.fit(noisy_imgs, original_imgs, 
                epochs=50, 
                batch_size=256)
```
Anomaly Detection
```python
# Calculate reconstruction error
reconstructions = autoencoder.predict(test_data)
mse = np.mean(np.power(test_data - reconstructions, 2), axis=1)

# Flag anomalies (high reconstruction error)
threshold = np.percentile(mse, 95)
anomalies = test_data[mse > threshold]
```
ğŸ“‚ Folder Structure
```css
day31-autoencoders/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ autoencoder_basic_keras.py
â”‚   â”œâ”€â”€ autoencoder_anomaly_detection.py
â”‚   â””â”€â”€ autoencoder_denoising_mnist.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ autoencoder_architecture.png
â”‚   â”œâ”€â”€ input_vs_reconstruction.png
â”‚   â”œâ”€â”€ anomaly_detection_autoencoder.png
â”‚   â”œâ”€â”€ autoencoder_latent_space_2D.png
â”‚   â”œâ”€â”€ reconstruction_loss_curve.png
â”‚   â””â”€â”€ denoising_effect_mnist.png
â””â”€â”€ README.md
```
ğŸ§  Summary
ğŸ”„ Autoencoders learn efficient data representations through reconstruction

ğŸ­ Different types solve specific problems (denoising, generation, etc.)

ğŸ“¦ Latent space captures essential features of input data

ğŸ•µï¸â€â™‚ï¸ Reconstruction error enables anomaly detection

ğŸ¨ VAEs enable controlled data generation

ğŸ” Previous Post
![âš–ï¸ Day 30 â€“ Imbalanced Data Techniques (SMOTE, Class Weights) ](.//day30-imbalanced-data-techniques)

ğŸ™Œ Stay Connected
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
â­ Star the GitHub Repo
"In the latent space, complexity finds its simplest truth."

The Autoencoder's Credo

â­ Star this repo to fuel our ML journey!
