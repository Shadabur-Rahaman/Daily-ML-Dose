# ğŸ§° Day 41 â€“ Regularization Techniques  
**#DailyMLDose** | Controlling Overfitting in Machine Learning

Regularization is the secret sauce that keeps machine learning models from memorizing the training data and failing on new data. It introduces **penalty terms** or **constraints** to prevent overfitting and improve generalization.

---

## ğŸ” Overview  
Today we cover:

- ğŸ“‰ L1 and L2 Regularization
- ğŸ”¥ Dropout in Neural Networks
- ğŸ¯ Early Stopping
- ğŸ§  Batch Normalization (acts as regularizer)
- ğŸ”„ Data Augmentation
- ğŸ“ˆ Visualization of Effects

---

## ğŸ–¼ï¸ Visuals

### 1. L1 vs L2 Penalty Intuition  
<img src="images/l1_vs_l2_penalty.png" width="600"/>

---

### 2. Dropout: Random Neuron Deactivation  
<img src="images/dropout_illustration.png" width="600"/>

---

### 3. Training Curves: Early Stopping  
<img src="images/early_stopping_curves.png" width="600"/>

---

### 4. BatchNorm as Regularizer  
<img src="images/batchnorm_regularization.png" width="600"/>

---

## ğŸ§ª Code Highlights

### âœ… L2 Regularization with PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(10, 1)
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # L2 regularization
```
âœ… L1 Regularization

```python

l1_lambda = 0.001
l1_penalty = sum(torch.sum(torch.abs(param)) for param in model.parameters())
loss = criterion(output, target) + l1_lambda * l1_penalty
```
âœ… Dropout in a Neural Network
```python

import torch.nn.functional as F

class DropoutNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)
```
âœ… Early Stopping Example

```python

best_loss = float('inf')
patience = 3
counter = 0

for epoch in range(100):
    train(...)
    val_loss = validate(...)
    if val_loss < best_loss:
        best_loss = val_loss
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered")
            break
```

ğŸ“ Folder Structure
```css

ğŸ“ day41-regularization-techniques/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ l1_regularization.py
â”‚   â”œâ”€â”€ l2_regularization.py
â”‚   â”œâ”€â”€ dropout_demo.py
â”‚   â”œâ”€â”€ early_stopping_example.py
â”‚   â””â”€â”€ batchnorm_regularizer.py
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ l1_vs_l2_penalty.png
â”‚   â”œâ”€â”€ dropout_illustration.png
â”‚   â”œâ”€â”€ early_stopping_curves.png
â”‚   â””â”€â”€ batchnorm_regularization.png
â””â”€â”€ README.md
```

ğŸ“š References
Deep Learning Book â€“ Ian Goodfellow, Chapter 7

CS231n Regularization

PyTorch Docs: Dropout

ğŸ” Navigation
â¬…ï¸ 
â¡ï¸ Day 42 â€“ Coming Soon

ğŸ”— Related Posts
![Day 40 â€“ Attention Mechanisms](https://github.com/Shadabur-Rahaman/Daily-ML-Dose/tree/main/day40-attention-mechanisms)

ğŸ™Œ Letâ€™s Connect!
ğŸ“ Connect With Me
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
---
â­ Star the GitHub Repo
---
ğŸ” Share this if it helped!
