# ğŸ“Š Day 5 â€“ Evaluation Metrics: Accuracy, Precision, Recall, F1, Specificity

Welcome to **Day 5** of #DailyMLDose!

Todayâ€™s concept is **Evaluation Metrics** â€” essential tools to judge how well your machine learning model performs.

---
## ğŸ—‚ï¸ Folder Structure â€“ day05-evaluation-metrics/
```
day05-evaluation-metrics/
|   â””â”€â”€demo/
|      â””â”€â”€  roc_vs_pr_curves.py     # Python script
â”œâ”€â”€ confusion_matrix.png
â”œâ”€â”€ metrics_formula.png
â””â”€â”€  README.md
---
```
## ğŸ“Œ Key Metrics in Classification

### âœ… Accuracy
The ratio of correctly predicted observations to the total observations.
```
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]
```
![Accuracy](Accuracy.jpg)

### ğŸ¯ Precision
How many of the predicted positives are actually positive?
```
\[
\text{Precision} = \frac{TP}{TP + FP}
\]
```
![Precision](precision.jpg)

### â™»ï¸ Recall (Sensitivity)
How many actual positives were correctly predicted?
```
\[
\text{Recall} = \frac{TP}{TP + FN}
\]
```
![Recall](recall.jpg)

### ğŸ›¡ï¸ Specificity (True Negative Rate)
How many actual negatives were correctly predicted?
```
\[
\text{Specificity} = \frac{TN}{TN + FP}
\]
```
![Specificity](specificity.jpg)
### âš–ï¸ F1-Score
Harmonic mean of Precision and Recall. Good when you need a balance.
```
\[
\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]
```
![Specificity](f1_score.png)
---
### Recall âœ… Accuracy ğŸ¯ Precision â™»ï¸ Recall (Sensitivity) âš–ï¸ F1-Score
![Accuracy, Precision, Recall, F1-Score](./Accuracy_Precision_Recall_f1.jpg)
---

## ğŸ“Š Confusion Matrix

A great way to visualize performance across all metrics:

![Confusion Matrix](Confusion_matrix.jpg)

|               | Predicted Positive | Predicted Negative |
|---------------|--------------------|--------------------|
| **Actual Positive** | TP                 | FN                 |
| **Actual Negative** | FP                 | TN                 |

---

## ğŸ“ When to Use What?

| Scenario                        | Best Metric        |
|---------------------------------|--------------------|
| Balanced classes                | Accuracy           |
| False positives are costly      | Precision          |
| False negatives are costly      | Recall             |
| You need a balance              | F1 Score           |

---

## ğŸ’¡ Real-World Analogy

ğŸ“¬ **Spam Email Classification**

- **High Precision**: Very few non-spam emails marked as spam  
- **High Recall**: Most spam emails are correctly caught  
- **F1 Score**: Tradeoff between missing spam and flagging valid emails

---

## ğŸ”— Code Snippet (sklearn)

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix
)

y_true = [1, 0, 1, 1, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1]

# Basic metrics
print("Accuracy:", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred))
print("Recall (Sensitivity):", recall_score(y_true, y_pred))
print("F1 Score:", f1_score(y_true, y_pred))

# Specificity from confusion matrix
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
specificity = tn / (tn + fp)
print("Specificity:", specificity)

```
ğŸ“ Visual Aids

ğŸ” Previous Post:
Day 4 â†’ [Train-Test Split](./day04-train-test-validation)

ğŸ“Œ Follow for more on [LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249/)
â­ Star the GitHub repo
ğŸ¯ Letâ€™s keep evaluating wisely!

