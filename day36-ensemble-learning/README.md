# ğŸ§© Day 36 â€“ Ensemble Learning Techniques

Welcome to **Day 36** of the #DailyMLDose challenge!

Today, we dive into **Ensemble Learning** â€” a powerful method where **multiple models** collaborate to produce better, more stable predictions.

---

## ğŸ” What is Ensemble Learning?

> Ensemble learning combines multiple base models to produce one optimal predictive model.

![Ensemble Overview](images/ensemble_methods_overview.png)

This method often **outperforms individual models** in terms of **accuracy, generalization**, and **robustness**.

---

## ğŸ“š Common Ensemble Techniques

### ğŸ§º Bagging (Bootstrap Aggregating)

> Trains multiple models on different **random subsets** of the training data (with replacement), then aggregates their predictions.

- Reduces variance.
- Works best with high-variance models like Decision Trees.

![Bagging Architecture](images/random_forest_bagging_architecture.png)

### ğŸš€ Boosting

> Trains models **sequentially**, where each new model focuses on the **errors** made by the previous ones.

- Reduces bias and variance.
- Examples: AdaBoost, Gradient Boosting, XGBoost, CatBoost.

![Bagging vs Boosting](images/bagging_vs_boosting.png)

### ğŸ§  Stacking

> Combines **diverse models** (base learners), then uses a **meta-learner** to learn from their combined outputs.

- Great for capturing non-linear interactions.
- Often used in ML competitions like Kaggle.

![Stacking Diagram](images/stacking_architecture_diagram.png)

### ğŸ¤ Blending

> Similar to stacking but uses a **validation set** (instead of cross-validation) to train the meta-learner.

- Quicker but more prone to overfitting.

![Blending Visual](images/blending_vs_stacking_visual.png)

### âš–ï¸ Voting

> Combines predictions from multiple models using **hard voting** (majority rule) or **soft voting** (average probabilities).

![Voting Types](images/voting_classifier_types.png)

---
## ğŸ“ Folder Structure

```css
ğŸ“ day36-ensemble-learning/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ bagging_random_forest.py
â”‚   â”œâ”€â”€ xgboost_vs_stacking.py 
â”‚   â”œâ”€â”€ catboost_classifier_blending.py 
â”‚   â””â”€â”€ voting_classifier_comparison.py
|       
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ bagging_vs_boosting.png
â”‚   â”œâ”€â”€ random_forest_bagging_architecture.png
â”‚   â”œâ”€â”€ stacking_architecture_diagram.png
â”‚   â”œâ”€â”€ xgboost_vs_stacking_chart.png
â”‚   â”œâ”€â”€ blending_vs_stacking_visual.png
â”‚   â”œâ”€â”€ catboost_blending_flow.png
â”‚   â”œâ”€â”€ voting_classifier_types.png
â”‚   â””â”€â”€ ensemble_methods_overview.png
â””â”€â”€ README.md
```
ğŸ–¼ï¸ Visual Understanding
ğŸ”¹ Bagging with Random Forest

ğŸ”¹ Boosting vs Bagging

ğŸ”¹ Stacking Architecture

ğŸ”¹ Blending vs Stacking

ğŸ”¹ Voting Classifier (Hard vs Soft)

ğŸ§ª Code Highlights
âœ… Bagging with Random Forest
```python
Copy
Edit
model = RandomForestClassifier(n_estimators=100, max_depth=5)
model.fit(X_train, y_train)
```
âœ… Stacking Models
```python
estimators = [
    ('lr', LogisticRegression()),
    ('svc', SVC(probability=True))
]
clf = StackingClassifier(estimators=estimators, final_estimator=GradientBoostingClassifier())
```
âœ… Blending with CatBoost
```python
cat = CatBoostClassifier(verbose=0)
cat.fit(X_train_blend, y_train)
ğŸ“Š Why Use Ensembles?
âœ… Reduce Variance
âœ… Reduce Bias
âœ… Boost Accuracy
âœ… Improve Generalization
```
ğŸ”— Previous Topics
![ğŸ” Day 35: Model Optimization & Tuning](https://github.com/Shadabur-Rahaman/Daily-ML-Dose/edit/main/day35-model-optimization)
![ğŸ§  Day 34 â†’ Advanced Boosting (XGBoost, CatBoost)](https://github.com/Shadabur-Rahaman/Daily-ML-Dose/tree/main/day34-advanced-boosting)
![ğŸ’¡ Day 33 â†’ Variational Autoencoders (VAEs)](https://github.com/Shadabur-Rahaman/Daily-ML-Dose/tree/main/day33-variational-autoencoders)


ğŸ™Œ Stay Connected
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
â­ Star the DailyMLDose GitHub Repo
ğŸ“˜ Let's learn ML, one dose a day!

ğŸ”¥ Summary
Ensemble methods are among the most powerful tools in the ML toolbox. Whether you're building competition-grade models or optimizing production pipelines â€” mastering them is a must! ğŸ’ª

ğŸ“… Stay Tuned
ğŸ“Œ Next Up: Day 37 â€“ ğŸ§  Explainable AI (XAI): Building Trust in ML Models

#ï¸âƒ£ #MachineLearning #EnsembleLearning #RandomForest #Boosting #Stacking #Blending #VotingClassifier #DataScience #DailyMLDose
