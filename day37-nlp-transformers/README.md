# ğŸŒ Day 37 â€“ Advanced NLP Applications  
ğŸ§  #DailyMLDose | Transformers, BERT & Zero-Shot Learning

Welcome to **Day 37** of #DailyMLDose!  
Todayâ€™s topic: Cutting-edge NLP powered by **Transformers**, **BERT**, and **Zero-shot Learning**.

---

## ğŸ“ Folder Structure
```css
day37-advanced-nlp/
â”œâ”€â”€ code/
â”‚ â”œâ”€â”€ transformer_basics.py
â”‚ â”œâ”€â”€ zero_shot_classification_hf.py
â”‚ â””â”€â”€ bert_embedding_example.py
â”‚
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ transformer_architecture.png
â”‚ â”œâ”€â”€ attention_is_all_you_need.png
â”‚ â”œâ”€â”€ bert_visualization.png
â”‚ â”œâ”€â”€ zero_shot_diagram.png
â”‚ â””â”€â”€ transfer_learning_nlp.png
â””â”€â”€ README.md
```

---

## ğŸ§  From RNNs to Transformers

Traditional NLP models like **RNNs** struggled with long dependencies.  
The **Transformer architecture** (2017) solved this with **self-attention** and parallelization.

![Transformer Architecture](images/transformer_architecture.png)  
![Attention Paper](images/attention_is_all_you_need.png)

---

## ğŸ” Transformer Architecture Key Points

- **Self-Attention** â†’ Relates each word to all others
- **Multi-Head Attention** â†’ Multiple attention heads learn diverse aspects
- **Feedforward Layers** â†’ Nonlinear processing of each token
- **Positional Encoding** â†’ Adds order to sequences

---

## ğŸ§ª BERT and Friends

**BERT (Bidirectional Encoder Representations from Transformers)** understands text in context â€” both left and right!

Itâ€™s pre-trained on large corpora and fine-tuned for:
- Sentiment analysis
- Question answering
- Named entity recognition

![BERT Visualization](images/bert_visualization.png)

### ğŸ”¤ Code: Extract Embeddings with BERT
```python
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

inputs = tokenizer("Transformers are powerful", return_tensors="pt")
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```
ğŸš€ Zero-Shot Learning (ZSL)
Zero-shot models predict without task-specific training â€” they match text to hypotheses.

Use case: Classify sentences into categories like finance, tech, sports without training on them!


ğŸ“Œ Hugging Face Pipeline Example
```python

from transformers import pipeline

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
result = classifier(
    "The launch of the new iPhone is tomorrow.",
    candidate_labels=["technology", "sports", "politics"]
)
print(result["labels"])
```
ğŸ”„ Transfer Learning in NLP
Modern NLP â†’ pretrained base model + task-specific fine-tuning.
This drastically reduces compute and improves results!


ğŸ“Š Summary Table
Model / Technique	Use Case	Tools/Libraries
BERT	Text classification	ğŸ¤— Transformers
GPT	Text generation	OpenAI, LangChain
ZSL	Label new categories	facebook/bart-large-mnli
Transformers	Universal NLP tasks	PyTorch, TensorFlow

ğŸ” Previous Day
![ğŸ“Œ Day 36 â€“ Ensemble Learning Techniques (Bagging, Stacking, Blending)](https://github.com/Shadabur-Rahaman/Daily-ML-Dose/tree/main/day36-ensemble-learning)

ğŸ™Œ Letâ€™s Connect!
ğŸ“ Connect With Me
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249)
â­ Star the GitHub Repo
ğŸ” Share this if it helped!

â€œLanguage is the bridge between humans and machines. And Transformers are the architects.â€
