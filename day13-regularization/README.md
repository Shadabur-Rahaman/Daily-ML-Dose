# ğŸ§® Day 13 â€“ Regularization in ML: L1, L2 & ElasticNet

Welcome to **Day 13** of #DailyMLDose!

Todayâ€™s concept is **Regularization**, a powerful technique to prevent **overfitting** in machine learning models by adding a penalty term to the loss function.

---

## ğŸ“Œ Why Regularization?

Machine learning models may perform **too well** on training data and generalize poorly to unseen data â€” this is **overfitting**.  
Regularization combats this by discouraging the model from learning overly complex patterns.

---

ğŸ“‚ Folder Structure â€“ `day13-regularization/`
```
day13-regularization/
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ difference_between_L1_&L2&elasticNet.png
â”‚ â”œâ”€â”€ elastic-net-regression-visually-equation-explained-1.webp
â”‚ â”œâ”€â”€ formulas_of_L1&L2&elasticNet.png
â”‚ â”œâ”€â”€ lasso_regression.png
â”‚ â”œâ”€â”€ Regularization_explanation.png
â”‚ â”œâ”€â”€ Regularization_formulas.jpeg
â”‚ â”œâ”€â”€ ridge-regression-in-machine-learning.webp
â”‚ â””â”€â”€ vsiuals_between_L1&L2&_elasticNet.png
â”œâ”€â”€ code/
â”‚ â””â”€â”€ regularization_examples.py
â””â”€â”€ README.md
```

---

## ğŸ§ª Types of Regularization

### ğŸ”¹ L1 Regularization (Lasso)
- Adds **absolute values** of coefficients as penalty.
- Can shrink some coefficients to **zero** â†’ performs **feature selection**.

ğŸ“¸  
![Lasso Regression](images/lasso_regression.png)

### ğŸ”¹ L2 Regularization (Ridge)
- Adds **squared values** of coefficients as penalty.
- Shrinks coefficients smoothly but doesnâ€™t eliminate them.

ğŸ“¸  
![Ridge Regression](images/ridge-regression-in-machine-learning.webp)

### ğŸ”¹ ElasticNet
- A combination of **L1 and L2** regularization.
- Balances sparsity and smooth shrinkage.

ğŸ“¸  
![ElasticNet Equation](images/elastic-net-regression-visually-equation-explained-1.webp)

---

## ğŸ“ˆ Visual Summary

ğŸ“Š L1 vs L2 vs ElasticNet Comparison:  
![Visual Differences](images/difference_between_L1_&_L2_&_elasticNet.png)  

ğŸ“Š Visual Explanation:  
![Formula Explanation](images/Regularization_explanation.png)  
![Regularization Formulas](images/Regularization_formulas.jpeg)  
![More Formulas](images/formulas_of_L1_&_L2_&_elasticNet.png)  
![Intuitive Visuals](images/vsiuals_between_L1_&_L2_&_elasticNet.png)

---

## ğŸ§  When to Use What?

| Regularization | Best For                              | Effect on Coefficients        |
|----------------|----------------------------------------|-------------------------------|
| L1 (Lasso)     | Feature selection, sparse models       | Some weights â†’ 0              |
| L2 (Ridge)     | Multicollinearity, smooth solutions    | All weights shrink uniformly  |
| ElasticNet     | Combines L1 and L2 advantages          | Sparse + smooth shrinkage     |

---

## ğŸ› ï¸ Sample Code

```python
from sklearn.linear_model import Lasso, Ridge, ElasticNet
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y)

lasso = Lasso(alpha=0.1)
ridge = Ridge(alpha=1.0)
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)

lasso.fit(X_train, y_train)
ridge.fit(X_train, y_train)
elastic.fit(X_train, y_train)
```
ğŸ” Previous:
Day 12 â†’ Gradient Descent Variants

ğŸ¨ Visual Credits:
Lasso/Ridge Visuals: @ml_diagrams

ElasticNet Formula: @ml_cheats

Visual Comparisons: @krishnaik06, @vijaykrishna101

ğŸ“Œ Stay Updated

- â­ Star the GitHub Repo
  
- ğŸ”— [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249/)  

Keep your models lean, clean, and regularized. ğŸš€
#DailyMLDose #MachineLearning #Regularization #Lasso #Ridge #ElasticNet #MLTips #ShadaburRahaman
