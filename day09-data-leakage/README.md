# ğŸ›‘ Day 9 â€“ Data Leakage in Machine Learning

Welcome to **Day 9** of #DailyMLDose!

Todayâ€™s topic is a critical one: **Data Leakage**, a hidden model killer that silently inflates your modelâ€™s performance and destroys its generalization ability.

---

## ğŸ“Œ What is Data Leakage?

Data leakage occurs when **information from outside the training dataset** is used to create the model. This causes your model to perform unrealistically well during training but fail in production.

---

## ğŸ” Common Types of Leakage

ğŸ“Œ **Target Leakage**:  
Features used for training contain **future or label-based information**.

ğŸ“Œ **Train-Test Contamination**:  
Information from the test set "leaks" into the training data, often during preprocessing.

ğŸ§  Visual â€“ Reasons for Data Leakage:  
![Data Leakage Types](data_leakage_reasons.png)

---

## âŒ Real-World Example of Leakage

```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# âš ï¸ Incorrect: Fit scaler on entire dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Leakage happens here!
X_train_leak = X_scaled[:len(X_train)]
X_test_leak = X_scaled[len(X_train):]

model = LogisticRegression()
model.fit(X_train_leak, y_train)
preds = model.predict(X_test_leak)

print("Leaked Accuracy:", accuracy_score(y_test, preds))
ğŸ“¸ Architecture That Causes Leakage:
```
## âœ… Correct Way to Avoid Leakage
```
# Correct: Fit scaler ONLY on training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model.fit(X_train_scaled, y_train)
preds = model.predict(X_test_scaled)

print("Correct Accuracy:", accuracy_score(y_test, preds))
```
ğŸ§  Why Itâ€™s Dangerous
False confidence from inflated metrics

Models break in production

Can be very subtle (especially with pipelines)

ğŸ§  Visual Summary:

ğŸ” How to Prevent It
Use pipelines (sklearn.pipeline) to encapsulate transforms

Always split your dataset before preprocessing

Double-check time-based features (timestamps, outcomes)

Use cross-validation properly (folds must be independent)

ğŸ§© Summary
Type	Cause	Example	Fix
Target Leakage	Label-related features	â€œDeliveredâ€ in training for delivery prediction	Remove or delay those features
Test Contamination	Preprocessing before split	Scaling entire dataset before split	Fit transforms only on training data

ğŸ” Previous:
[Day 8 â†’ Cross Validation: K-Fold vs Stratified](../day08-cross-validation)

ğŸ¨ Visual Credits:
Venn + Concept: @ml_diagrams

Architecture & Scaling Flow: @MLTwist

Visual Summary: @krishnaik06

ğŸ“Œ Stay Connected:

â­ Star the GitHub Repo

ğŸ”—  [Follow Shadabur Rahaman on LinkedIn](https://www.linkedin.com/in/shadabur-rahaman-1b5703249/)  
